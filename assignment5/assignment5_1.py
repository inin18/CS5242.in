# -*- coding: utf-8 -*-
"""assignment5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YHTHZhXsbtMTXVAXrhSoqXNCc2RCXkyi

## Setup

Start by running the cell below to set up all required software.

# Welcome to CS 5242 **Assignment 5**


In this assignment, we aim to:
1. Learn the Muon optimizer, and implement it.
2. training a Qwen3-0.6B on Allenai/c4 with the implemented Muon optimizer.
3. Learn to use the huggingface.trainer to train the model.

You can use Colab/your personal GPUs/any other resources to run our experiments.

In the case that you are not familiar with Colab: Colab is a hosted Jupyter notebook service that requires no setup to use, while providing access free of charge to computing resources including GPUs:
1. Login Google Colab https://colab.research.google.com/
2. In this assignment, We **need GPU** to training the CNN model. You may need to **choose GPU in Runtime -> Change runtime type -> Hardware accerator**

If your GPU has limited memories, you can set the per_device_train_batch_size and max_length to adjust the memory usage.

### **Grades Policy**

We have 10 points for this homework. 15% off per day late, 0 scores if you submit it 7 days after the deadline.

I'll give you two weeks for this assignment.

For this assignment, there are a lot of freedom for you to train the model. In the final grading stage, I will focus on:

1. the implementation of Muon optimizer.
2. the final loss of the training (full score for final loss less than ~1~ 5. You have to leave the logging info of your training loss in your submission).

The training requires:

1. use the muon you implemented.
2. use the Qwen3-0.6B model/or other less strong model, but please do not use models larger than 1B. My standard for passing is not strict, so even less strong model is enough.
3. It is not allowed to load the model from pretrained!!! you need to follow the code to load the model from scratch.
4. Any other hyper-parameters are free to modify.

For other questions, please contact me.

I will briefly introduce muon in the tutorial.


### **Contact**

Please feel free to contact us if you have any question about this homework or need any further information.

TA Email: E1154541@u.nus.edu

Import the neccesary libraries.
"""

# !pip install loguru

import os
import math
import random

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import transformers

from transformers import (
    TrainingArguments,
    DataCollatorForLanguageModeling,
    AutoModelForCausalLM, AutoTokenizer, AutoConfig,
    Trainer
)
from loguru import logger
import datasets
transformers.logging.set_verbosity_error()
print("GPU availability: ",torch.cuda.is_available()) 
random.seed(0)
np.random.seed(0)
torch.manual_seed(0)

# !nvidia-smi

torch.cuda.is_available()

"""Everything is ready, you can move on and ***Good Luck !*** üòÉ

# Step 1

Implement the Muon optimizer.
"""

import torch
from torch import nn
from torch.optim import Optimizer

def zeropower_via_newtonschulz5(G, steps: int):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2  # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    # X = G.to(dtype=G.dtype)
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        # === Complete the code
        A = X @ X.mT
        X = a * X + (b * A + c * (A @ A)) @ X
        X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
        # === Complete the code
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


def muon_update(grad, momentum, beta=0.95, ns_steps=5, nesterov=True):
    # momentum update, please consider the nesterov as True
    # ===== Complete the code =====
    momentum.mul_(beta).add_(grad)
    if nesterov:
        update = grad +beta * momentum
    else:
        update = momentum

    # ===== Complete the code =====
    update = zeropower_via_newtonschulz5(update, steps=ns_steps)
    update *= max(1, grad.size(-2) / grad.size(-1))**0.5
    return update

def adam_update(grad, buf1, buf2, step, betas, eps):
    # ===== Complete the code =====
    buf1.mul_(betas[0]).add_(grad, alpha = 1-betas[0])
    # buf2.mul_(betas[1]).add_((1-betas[1])*grad*grad)
    buf2.mul_(betas[1]).addcmul_(grad, grad, value=1-betas[1])
    mt = buf1/(1-betas[0]**step)
    vt = buf2/(1-betas[1]**step)

    if torch.isnan(mt).any() or torch.isinf(mt).any():
        print(f"[NaN in mt] step={step}, min={mt.min().item()}, max={mt.max().item()}")

    if torch.isnan(vt).any() or torch.isinf(vt).any():
        print(f"[NaN in vt] step={step}, min={vt.min().item()}, max={vt.max().item()}")

    update = mt/(vt.sqrt()+eps)
    update = torch.nan_to_num(update, nan=0.0, posinf=0.0, neginf=0.0)

    if torch.isnan(update).any() or torch.isinf(update).any():
        print(f"[NaN in update] step={step}, min={update.min().item()}, max={update.max().item()}")


    # ===== Complete the code =====
    return update

class SingleDeviceMuonWithAuxAdam(torch.optim.Optimizer):
    """
    Non-distributed variant of MuonWithAuxAdam.
    """

    def __init__(self, param_groups):
        for group in param_groups:
            assert "use_muon" in group
            if group["use_muon"]:
                # defaults
                group["lr"] = group.get("lr", 0.02)
                group["momentum"] = group.get("momentum", 0.95)
                group["weight_decay"] = group.get("weight_decay", 0)
                assert set(group.keys()) == set(["params", "lr", "momentum", "weight_decay", "use_muon"])
            else:
                # defaults
                group["lr"] = group.get("lr", 3e-4)
                group["betas"] = group.get("betas", (0.9, 0.95))
                group["eps"] = group.get("eps", 1e-7)
                group["weight_decay"] = group.get("weight_decay", 0)
                assert set(group.keys()) == set(["params", "lr", "betas", "eps", "weight_decay", "use_muon"])
        super().__init__(param_groups, dict())

    @torch.no_grad()
    def step(self, closure=None):

        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        for group in self.param_groups:
            if group["use_muon"]:
                for p in group["params"]:
                    if p.grad is None:
                        # continue
                        p.grad = torch.zeros_like(p)  # Force synchronization
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(p)
                    update = muon_update(p.grad, state["momentum_buffer"], beta=group["momentum"])
                    p.mul_(1 - group["lr"] * group["weight_decay"])
                    p.add_(update.reshape(p.shape), alpha=-group["lr"])
            else:
                for p in group["params"]:
                    if p.grad is None:
                        # continue
                        p.grad = torch.zeros_like(p)  # Force synchronization
                    # if torch.isnan(p.grad).any():
                        # print(f"Before Adam-NaN in grad for {id(p)}")
                    state = self.state[p]
                    if len(state) == 0:
                        state["exp_avg"] = torch.zeros_like(p)
                        state["exp_avg_sq"] = torch.zeros_like(p)
                        state["step"] = 0
                    state["step"] += 1

                    update = adam_update(p.grad, state["exp_avg"], state["exp_avg_sq"], state["step"], group["betas"],
                                         group["eps"])
                    p.mul_(1 - group["lr"] * group["weight_decay"])
                    p.add_(update, alpha=-group["lr"])

        return loss

"""# Step 2

Prepare data and model
"""

# Try different configs and make the training loss less than 1 at the end of learning.
# All configs can be changed
# ===== Complete the code =====
class Config:
    num_training_steps = -1
    # total batch size = per_device_train_batch_size * gradient_accumulation_steps,
    # I suggest you to set the total batch size no less than 32,
    # for example, per_device_train_batch_size = 2, gradient_accumulation_steps = 16
    per_device_train_batch_size = 2
    gradient_accumulation_steps = 8
    learning_rate = 2e-5
    weight_decay = 0.01
    warmup_steps = 200 #500
    logging_steps= 5
    remove_unused_columns= True
    dataloader_num_workers= 4 # I suggest 4, you can try other values
    seed=42
    # I suggest bf16, you can try fp16, but do not use both. bf16 actually not work for some GPUs, in that case, you may need to use fp16.
    # I conduct my own experiments on bf16, and it works for me. However, I have not tested fp16. So if there are any problem with fp16, please let me know.
    fp16=False
    bf16=True
    # I suggest 512 if you have enough GPU memory, 256 if you have limited GPU memory
    max_length= 256
    max_grad_norm=1.0 # I suggest 1.0, you can try other values
    report_to="none", # optional, if you want to use wandb it is also ok
    run_name="assignment5",
    gradient_checkpointing=True, # optional, enable to save GPU memory, but if you have enough GPU memory, you can disable it.
    dataloader_drop_last=False, # Optional: if the last batch is not full, set it to True
    include_num_input_tokens_seen=True,
# ===== Complete the code =====

def set_random_seed(random_seed: int = 42):
    # Setting random seed for all
    random.seed(random_seed)
    torch.manual_seed(random_seed)
    torch.cuda.manual_seed(random_seed)
    torch.cuda.manual_seed_all(random_seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(random_seed)
    logger.info(f"Set random seed to {random_seed}")
    return random_seed

config = Config()
assert not (config.bf16 and config.fp16), "fp16 and bf16 cannot be used together"
# Set seeds
set_random_seed(config.seed)
# Load model and tokenizer
# raw_train_dataset = datasets.load_dataset("allenai/c4", "en", split="train", streaming=True)
# raw_train_dataset = raw_train_dataset.take(16)
raw_train_dataset = datasets.load_from_disk(r"D:\Workspace\project1\CS5242.in\assignment5\data\c4_subset_6400")

# load the model and tokenizer, we train the model from scratch!
# you are not allowed to load the model from pretrained.
# model_config = AutoConfig.from_pretrained("facebook/opt-125m") # you can also use other open-source model. "Qwen/Qwen3-0.6B"
# # model = AutoModelForCausalLM.from_pretrained(model_config)
# model = AutoModelForCausalLM.from_config(model_config) # you can also use other open-source model.
# tokenizer = AutoTokenizer.from_pretrained("facebook/opt-125m") # you can also use other open-source model.

model_path = r"D:\Workspace\project1\CS5242.in\assignment5\data\qwen2.5-0.5b"
print('Model Path = ', model_path)

model_config = AutoConfig.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_config(model_config)

# # Ê£ÄÊü•Âπ∂ÈáçÁΩÆÊ®°ÂûãÂèÇÊï∞
# def reset_parameters(m):
#     if isinstance(m, (nn.Linear, nn.Embedding)):
#         torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)  # OPT ÈªòËÆ§ÂàùÂßãÂåñ
#         if hasattr(m, "bias") and m.bias is not None:
#             torch.nn.init.zeros_(m.bias)
#     elif isinstance(m, nn.LayerNorm):
#         nn.init.ones_(m.weight)
#         nn.init.zeros_(m.bias)


# # Â¶ÇÊûúÊ®°ÂûãÈáåÊúâ NaNÔºåÂ∞±ÈáçÁΩÆ
# for name, param in model.named_parameters():
#     if torch.isnan(param).any():
#         print(f"NaN detected in {name}, resetting all params...")
#         model.apply(reset_parameters)
#         break

# Tokenization function
def tokenize_function(examples):
    texts = examples["text"]
    return tokenizer(
        texts,
        truncation=True,
        padding=False,
        max_length=config.max_length,
        return_attention_mask=True,
        return_special_tokens_mask=False,
    )
train_dataset = raw_train_dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=raw_train_dataset.column_names
)
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
    pad_to_multiple_of=8,
    return_tensors="pt",
)

# for i, batch in enumerate(train_dataset):
#     if i < 16:
#         print(i, tokenizer.decode(batch["input_ids"]))
print(model)

# Build the Muon optimizer
# ===== Complete the code =====
muon_params_list = []
adam_params_list = []
patterns = [
    "emb",
    "norm",
    "lm_head",
    "bias",
    "wte",
    "wpe",
    "output",
    "conv",
    "rotary",
]
for name, param in model.named_parameters():
    if not param.requires_grad:
        continue
    # pick out those parameters that are not in the patterns and have 2-D shape, we only apply the muon optimizer to those parameters, while leave the rest of the parameters to adam optimizer
    # ===== Complete the code =====
    # use_muon = param.ndim>=2 and not list(filter(lambda x: x in name, patterns))
    use_muon = param.ndim>=2 and not any(p in name.lower() for p in patterns)
    # use_muon= False
    # ===== Complete the code =====
    param.use_muon = use_muon
    if use_muon:
        muon_params_list.append(param)
    else:
        adam_params_list.append(param)

params_groups = [
    {
        "params": muon_params_list,
        "use_muon": True,
        "lr": config.learning_rate,
        "momentum": 0.95,
        "weight_decay": config.weight_decay,
    },

    {
        "params": adam_params_list,
        "use_muon": False,
        "lr": config.learning_rate,
        "betas": (0.9, 0.999),
        "eps": 1e-8,
        "weight_decay": config.weight_decay,
    },
]

from transformers import get_scheduler
optimizer = SingleDeviceMuonWithAuxAdam(params_groups)
# you can also use other scheduler
scheduler = get_scheduler(
    "cosine",
    optimizer,
    num_warmup_steps=config.warmup_steps,
    num_training_steps=config.num_training_steps,
)

"""# Step 3

Training the model
"""

# Setup training arguments
# from transformers import ProgressCallback
# from transformers.integrations import NotebookProgressCallback
training_args = TrainingArguments(
    max_steps=config.num_training_steps,
    per_device_train_batch_size=config.per_device_train_batch_size,
    gradient_accumulation_steps=config.gradient_accumulation_steps,
    logging_steps=config.logging_steps,
    remove_unused_columns=config.remove_unused_columns,
    dataloader_num_workers=config.dataloader_num_workers,
    max_grad_norm=config.max_grad_norm,
    seed=config.seed,
    fp16=config.fp16,
    bf16=config.bf16,
    report_to=config.report_to[0],
    run_name="Assignment5",
    gradient_checkpointing=config.gradient_checkpointing,
    dataloader_drop_last=bool(config.dataloader_drop_last),                    # Optional: if the last batch is not full, set it to True
    include_num_input_tokens_seen=config.include_num_input_tokens_seen,
)

# Data collator

# === Sanity Check ===
batch = next(iter(torch.utils.data.DataLoader(train_dataset, batch_size=1, collate_fn=data_collator)))

# ËæìÂÖ•ËåÉÂõ¥
print("vocab_size:", model.config.vocab_size)
print("input_ids min/max:", batch["input_ids"].min().item(), batch["input_ids"].max().item())
print("labels min/max:", batch["labels"].min().item(), batch["labels"].max().item())

# embedding ËæìÂá∫
emb_out = model.get_input_embeddings()(batch["input_ids"])
print("Embedding output:", emb_out.min().item(), emb_out.max().item(), "has_nan:", torch.isnan(emb_out).any().item())

# # forward hookÔºàÊ£ÄÊü•ÂâçÂá†Â±Ç LinearÔºâ
# def debug_hook(module, inp, out):
#     print(f"[{module.__class__.__name__}] min={out.min().item():.4f}, max={out.max().item():.4f}, has_nan={torch.isnan(out).any().item()}")

# for name, mod in list(model.named_modules())[:20]:
#     if isinstance(mod, torch.nn.Linear):
#         mod.register_forward_hook(debug_hook)

# # ÊúÄÁªà logits Âíå loss
# outputs = model(**{k: v.to(model.device) for k, v in batch.items()})
# print("Final logits:", outputs.logits.min().item(), outputs.logits.max().item(), "has_nan:", torch.isnan(outputs.logits).any().item())
# print("Loss:", outputs.loss.item())
# # === Sanity Check ÁªìÊùü ===


# Create trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=data_collator,
    optimizers=(optimizer, scheduler)
)
# trainer.remove_callback(ProgressCallback)
# trainer.add_callback(NotebookProgressCallback)
# Start training
logger.info("Starting training...")
logger.info(f"Training steps: {config.num_training_steps}")
logger.info(f"Batch size: {config.per_device_train_batch_size}")
logger.info(f"Gradient accumulation steps: {config.gradient_accumulation_steps}")
logger.info(f"Learning rate: {config.learning_rate}")

# if config.resume_from_checkpoint:
#     logger.info(f"Resuming from checkpoint: {config.resume_from_checkpoint}")
#     trainer.train(resume_from_checkpoint=config.resume_from_checkpoint)
# else:

# for step, batch in enumerate(trainer.get_train_dataloader()):
#     batch = {k: v.to(model.device) for k,v in batch.items()}
#     print(f"[Check step={step}] "
#           f"input_ids max={batch['input_ids'].max().item()}, "
#           f"labels max={batch['labels'].max().item()}")
#     outputs = model(**batch)
#     print(f"loss={outputs.loss.item()}, logits has nan={torch.isnan(outputs.logits).any()}")
#     if step > 5:
#         break
if __name__ == "__main__":
    trainer.train()
    logger.info("Training completed!")

# !top -n 1 | grep python